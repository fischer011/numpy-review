# -*- coding: utf-8 -*-
"""Numpy_math_review.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUe0kC8wfmV2xlvDO_0zRlTNJR4nQfPH
"""

import numpy as np
v = np.array([1,1])
v
v.shape

m = np.array([[3,2], [3,2]])
m
m[0,[0]]

import matplotlib.pyplot as plt

plt.xlim(0, 3)
plt.ylim(0, 3)

d0=m[0]
d1=m[1]
dx=d0[0]
dy=d0[1]

plt.arrow(0, 0, dx, dy, head_width=0.05, head_length=0.1)

plt.xlim(0, 3)
plt.ylim(0, 3)

plt.arrow(0, 0, v[0], v[1], head_width=0.05, head_length=0.1)

"""L2 norm formula is Euclidean distance (hypotenuse of triangle formed by x length and y length of vector)

$$||x||_{2} = \sqrt{x_{1}^{2} + x_{2}^{2} +\cdots + x_{n}^{2}}$$

For v: 1^2 + 1^2 = 2;
sqrt(2) ~ 1.41
"""

np.sum(v ** 2)

np.sqrt(np.sum(v ** 2))

v3 = np.array([0, 1, 2])

fig1 = plt.figure()
ax = fig1.add_subplot(111, projection='3d')

ax.set_xlim([0, 3])
ax.set_ylim([0, 3])
ax.set_zlim([0, 3])

ax.quiver(0, 0, 0, v3[0], v3[1], v3[2], length=1)

fig2 = plt.figure()
ax = fig2.add_subplot(111, projection='3d')

ax.set_xlim([0, 3])
ax.set_ylim([0, 3])
ax.set_zlim([0, 3])

ax.quiver(0, 0, 0, v3[0], v3[1], v3[2], length=1)

v5 = np.array([0, 1, 2, 3, 4])

"""Scaling vectors"""

v_half = np.array([2,1]) * 0.5
v_half

"""Adding vectors"""

v_add  = np.array([1,1]) + v_half
v_add

"""Basis vectors

Canonical basis vectors:

0,1

1,0
"""

v1 = np.array([0,1])
v2 = np.array([1,0])

# Can use basis vectors to reach any point in 2D space
# 0.3, 1

v1 + v2 * 0.3

# Basis vectors are orthogonal to each other
# So dot product equals zero

np.dot(v1,v2)

"""You can change basis vectors

Important usage in ML and DL
"""

M = np.array([[0,1,2], [1,0,2], [3,1,0]])
M

"""Matrix variable is uppercase by convention in programming and math"""

M.shape

"""Linear regression formula

$\hat{y} = wx + b$
"""

import pandas as pd

# Read in the data
data = pd.read_csv("/content/clean_weather.csv", index_col=0)
# Fill missing data with past data
# Fill NA/NaN values by propagating the last valid observation to next valid.
data = data.ffill()

data.head(5)

"""$\hat{y} = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$

---


Matrix multiplication
"""

X = data[['tmax', 'tmin', 'rain']].iloc[:3].to_numpy()
X

X.shape

X_T = X.T
X_T

w = np.array([0.7, 0.3, 1])
w.shape

"""Reshaping w vector (1D) to W matrix with 3 rows and 1 column.

Reshape requires same # of elements in original array and new array.
"""

W = w.reshape(3,1)
W

"""Reshaping W matrix to play with transpose."""

W_as_one_row = W.reshape(1,3)
print(W_as_one_row)

"""@ is np.matmul shorthand

https://numpy.org/doc/2.1/reference/generated/numpy.matmul.html#numpy.matmul

Playing with transpositions
"""

w_t = w.T

print(X_T.shape)
print(w_t.shape)
m_t = X_T @ W_as_one_row.T
m_t

m_o = w @ X
m_o.shape

m_new = m_o.reshape(1,3)
m_new

"""np.allclose

Returns True if two arrays are element-wise equal within a tolerance.

**KEEP IN MIND THE SHAPE MUST ALIGN (e.g., (1,4) is not the same as (4,1)).
https://numpy.org/doc/2.3/reference/generated/numpy.allclose.html#numpy-allclose
"""

np.allclose(X_T @ W_as_one_row.T, m_new.T)

X @ W

np.dot(X,W)

np.dot([2j, 2 + 3j], [2j, 3j])

np.dot([2,3], [2,3])

np.dot([1j,1j], [1j,1j])

b = np.array([10])
# b is bias given for example, not calculated

Y = X @ W + b
Y

"""Slope-intercept formula relation

$y = mx + b$

---

Normal Equation Method

$$W = (X^T X)^{-1} X^T Y$$

---
Identity matrix

np.eye

Return a 2-D array with ones on the diagonal and zeros elsewhere.

https://numpy.org/doc/2.3/reference/generated/numpy.eye.html#numpy.eye

Inverse matrices

np.linalg.inv

DETERMINANT (also called AREA) is the SCALING FACTOR

A matrix has an inverse (is invertible) if and only if its determinant is non-zero.

Cannot invert a singular matrix because determinant is 0.  More simply, determinant is divisor & can't divide by 0. Inverted matrix will be undefined.
"""

#n = np.linalg.inv(X)
#n

#norm = np.linalg.inv((X_T @ X)) @ X_T @ Y
#norm

"""Ridge regression to solve problem of singular matrix.  We add a small 'ridge' to the diagonal."""

id_m = 0.1 * np.eye(3)
id_m

"""Test X.shape

index 0 rows

"""

id_test = 0.1 * np.eye(X.shape[0])
id_test

oh = X.shape[0]
oh

a = np.array([[1, 2], [3, 4], [5, 6]])

print(a)
print(a.shape[0])
print(a.shape[1])

x_ridge = (X + id_m)
x_ridge

inv = np.linalg.inv(x_ridge)
inv

"""Using allclose again to check if matrices are the same aside from rounding off."""

check_close = np.allclose(x_ridge @ inv, np.eye(3))
check_close

"""Using normal equation to calculate weights (W)

$$W = (X^T X)^{-1} X^T Y$$
"""

Y = data[['tmax_tomorrow']].iloc[:3].to_numpy()
W = np.linalg.inv(X.T @ X + id_m) @ X.T @ Y
# = np.linalg.inv(X.T @ X + 0.1 * np.eye(X.shape[0])) @ X.T @ Y
W

Y

X @ W

c = np.ones((5,1))
c

d = np.ones((1,1))
d

e = c + d
e

f = np.array([[2,4,7,8], [6,8,5,2], [10,12,7,6]])
print(f.shape)
print(len(f))

"""Derivative is slope of tangent line to the curve of the function at a specific point.

---
Finite differences method

$\frac{y^2 - y^1}{x^2 - x^1}$

---
Derivatives important for training neural networks and backpropagation.

Plotting functions and calculating derivatives.
"""

import matplotlib.pyplot as plt

x = np.arange(-60, 60, 0.1)
fx = x ** 3

plt.plot(x, fx)

x1 = 40 - 1e-7
x2 = 40 + 1e-7

y1 = x1 ** 3
y2 = x2 ** 3

slope = (y2 - y1)/(x2 - x1)
slope

"""Checking against my derivitave calculation:

$f(x) = x^3$

$f'(x) = 3x^2$
"""

x = 40
der = 3 * x ** 2
der